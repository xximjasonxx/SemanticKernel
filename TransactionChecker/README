# Transaction Query Lab

## Purpose
This lab guides you through a representation of a common AI-related business scenario which involves performing prompt-based queries against a custom dataset. In this case, an Excel file is provided which contains a list of transactions ranging from 11/14/2021 to 7/9/2024, approximately 2800 items. The lab then shows how to construct a code base to support RAG (Retrieval Augmented Generation) queries to query this custom dataset.

## Our Process
We will perform the following steps to execute this lab:
1. Store the Transactions Excel file is a readable location
2. Use an ETF process to move the data into a datastore, Cosmos for this lab
3. Implement a console application with C# to perform queries against our data

Having the data is Cosmos reflects the concept of customer data being in a private datastore be it SQL, Data Lake, Redis, or some other system.

## Technical Notes/Warnings
For simplicity this lab leverages Account Keys for both Storage Account and Cosmos. While appropriate for this directed lab, managed identities should be used for *Production* scenarios as a safeguard against storing sensitive data.

## Prequisites
Prior to beginning development, your Azure subscription should have a Azure OpenAI instance deployed with a generative model deployed

Instructions: [https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource?pivots=web-portal](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource?pivots=web-portal)

Have .NET 8 installed on your development machine.

Download .NET 8: [https://dotnet.microsoft.com/en-us/download/dotnet/8.0](https://dotnet.microsoft.com/en-us/download/dotnet/8.0)

Have a code editor installed. For this demo, screenshots and references will be using VSCode. But this lab does not have an IDE-specific requirement

## Preparation
The first goal is to get the data into Cosmos. There are a number of ways to do this but we will use Azure Data Factory to perform the ETL operation. As it is not unlikely that the data you may be working with is significantly larger than this dataset, having skills around ETL are beneficial.

### Create the Cosmos Instance
Instructions: [https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/quickstart-portal](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/quickstart-portal)
- **API**: NoSQL
- **Capacity**: Serverless

As this lab is not intended for *Production* use, it is advised you disable redundancy and replication options.

Create a **database** and **collection**. Specify the **Partition key** as **/Category**
![Create the Cosmos data store elements](./readme_images/cosmos-screen1.png)

Naming of Cosmos databse and collection are flexible. The above is not a recommendation for naming or partitioning strategy.

### Create the Data Factory Instance (Optional)
Instructions: [https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory](https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory)
The Data Factory will be used to move our data from the Excel file into CosmosDB. This is **NOT** required as you could use a Console script or something else to move the data. We are using it here as it closely follows the business use case we are representing.

## Move the Data
### Create a Storage Account and Upload the Excel file
1. Create a Storage Account ([here](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-create?tabs=azure-portal))
2. Create a container
3. Upload the **transactions.xlsx** file to the container - [instructions](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-portal)

The file will be accessed via an Access Key provided to Azure Data Factory. Do **NOT** enable public access

### Create a Pipeline to Copy the Data
1. Access your Azure Data Factory instance - select **Launch Studio**
2. Ensure the **Author** button is selected in the left hand navigation
3. From **Pipelines**, click the **...** and select the **New Pipeline** option
![Select New Pipeline from the Pipelines menu](./readme_images/datafactory-screen1.png)

4. On the far right, provide a name for the pipeline. In our example I will use **MyPipeline** for the name
5. Expand the **Move and transform** category
6. Drag **Copy data** to the authoring space
![Authoring space with Copy Data task](./readme_images/datafactory-screen2.png)

### Setup the Source Configuration
1. Select the **Source** tab from the lower section
2. Click **+ New** for **Source dataset**
3. Select the **Azure** tab from the right flyout menud and then pick **Azure Blob Storage** press **Continue**
![Select Azure Blob Storage as the source for our data](./readme_images/datafactory-screen3.png)

4. Choose the format - pick **Excel** and press **Continue**
5. From the **Linked service** select **+ New**
6. Fill in the details for the Storage Account which holds the Excel file - press **Create**
![Configure the Storage Account source](./readme_images/datafactory-screen4.png)

7. Use the **Browse file...** feature to select the Excel file from Blob Storage
8. Make sure **First row as header** is checked and select **Sheet1**.
![Fill in the properties for the Source dataset](./readme_images/datafactory-screen5.png)
9. Press **Ok**
![The source configured](./readme_images/datafactory-screen6.png)

### Setup the Destination Configuration
1. Select the **Sink** tab from the lower section
2. Click **+ New** for **Sink dataset**
3. Select **Azure Cosmos DB for NoSQL** and click **Continue**.  
![Select Cosmos as the destination](./readme_images/datafactory-screen7.png)

4. From the **Linked service** select **+ New**
5. Fill in the details for the Cosmos database created earlier - press **Create**
6. Select the **Container/Collection** to target - press **Ok**.  
![Set Properties for Cosmos connection](./readme_images/datafactory-screen8.png)
7. Ensure the **Write behavior** (beneath **Sink dataset**) is set to **Upsert**

### Configure the Mapping
1. Select **Mapping** tab from the lower section
2. Click **Import schemas**
3. Configure the mapping as shown below:
![The final mappings](./readme_images/datafactory-screen9.png)
**Note:** Past *Date* no mapping is being done, this is intentional

### Trigger a Pipeline Run
1. From the upper bar select **Add trigger**
2. Select **Trigger now**
This will cause the the factory to run.

3. Once complete validate the data in Cosmos using **Data explorer**
4. If satisfied, click **Publish all** at the top to persist changes
5. Close **Data Factory Studio**

## Create the Query Program
Prior to writing this code you must have the following information readily available:
- The name of a GPT deployment in Azure OpenAI
- The Endpoint and Key for the Azure OpenAI service you will be using

### What is Semantic Kernel?
Prompt-engineering is the discipline of understanding how to talk to LLMs (Large Language Models) that GenAI is built on. In theory, the better the prompt the better the information returned. While this is true, complex (or multi-step) AI applications quickly make simple propmt engineering overburdened. Semantic Kernel, and other frameworks like it, introduce an *Agent-based* approach.

The idea of using *agents* is that each agent can be specialized or *grounded* a certain way that helps it perform a specific role in achieving a goal. Our lab will create a Cosmos agent that perform queries based on the given request. Semantic Kernel will use the referenced GPT model to *understand* what the various pieces in the code base *do* and pull them together to solve the problem.

### Create the application
We will create a Console application for simplicity. The Console application will start up, ask the user to provide a request, and then return a response.
1. Execute `dotnet new console --output QueryTransactions`
2. Add the NuGet packages
    - `dotnet add package Microsoft.SemanticKernel`
    - `dotnet add package Microsoft.SemanticKernel.Planners.Handlebars --version 1.16.1-preview`
    - `dotnet add package Microsoft.Extensions.Configuration.UserSecrets`
    - `dotnet add package Microsoft.Azure.Cosmos`
 
3. Initialize .NET Secrets Manager - `dotnet user-secrets init`
4. Add Sensitive data to the Secret Manager
   - `dotnet user-secrets set OpenAIEndpoint "SOME_ENDPOINT"`
   - `dotnet user-secrets set OpenAIApiKey "SOME_KEY"`
   - `dotnet user-secrets set CosmosConnectionString "SOME_CONNECTION_STRING"`

6. In the **Program.cs** file paste the following code:
```
using Microsoft.Extensions.Configuration;
using Microsoft.Extensions.DependencyInjection;
using Microsoft.SemanticKernel;
using Microsoft.SemanticKernel.Planning.Handlebars;
using TransactionChecker;
using TransactionChecker.Plugins;

Console.Write("Enter a request: ");
var requestString = Console.ReadLine() ?? string.Empty;

var config = new ConfigurationBuilder()
    .AddUserSecrets<Config>()
    .Build();

var builder = Kernel.CreateBuilder();
builder.Services.AddSingleton<IConfiguration>(config);
builder.AddAzureOpenAIChatCompletion(
    deploymentName: "gpt-4o-gs-deployment",
    endpoint: config["OpenAIEndpoint"],
    apiKey: config["OpenAIApiKey"]);

var kernel = builder.Build();

#pragma warning disable // Suppress the diagnostic messages
var planner = new HandlebarsPlanner(
    new HandlebarsPlannerOptions() { AllowLoops = true });

var plan = await planner.CreatePlanAsync(kernel, requestString);
var planResult = await plan.InvokeAsync(kernel, new KernelArguments());

Console.WriteLine(planResult);
```
Running this code will not produce the results we want, that is because we have added no intelligence to the application.

### Add the Cosmos Plugin
Plugins in Semantic Kernel and the blocks that *do things*. The **Cosmos Plugin** is an example of a *code-based* plugin. That is a plugin that exposes methods written in a supported language (C# in this case) that carries out tasks.

1. Create a folder called **Plugins**
  - This is not necessarily required by follows the convention suggested by Semantic Kernel documentation

2. Create a class **CosmosPlugin** in this folder
3. Paste the following code into this class
```
public sealed class CosmosPlugin
{
    private readonly IConfiguration _configuration;
    public CosmosPlugin(IConfiguration configuration)
    {
        _configuration = configuration;
    }
    [KernelFunction("byCategoryInDateRange")]
    [Description("Get sum of amount for transactions by category for a date range")]
    public async Task<decimal> ByCategoryInDateRange(
        [Description("The category to filter by")]string category,
        [Description("The start date of the date range")]string startDate,
        [Description("The end date of the date range")]string endDate)
    {
        var cosmosClient = new CosmosClient(_configuration["CosmosConnectionString"]);
        var database = cosmosClient.GetDatabase("transactions");
        var container = database.GetContainer("byCategory");
        var query = new QueryDefinition("SELECT VALUE SUM(c.Amount) FROM c WHERE c.Category = @category AND c.Date >= @startDate AND c.Date <= @endDate")
            .WithParameter("@category", category)
            .WithParameter("@startDate", startDate)
            .WithParameter("@endDate", endDate);
        var queryIterator = container.GetItemQueryIterator<decimal>(query);
        if (queryIterator.HasMoreResults)
        {
            var response = await queryIterator.ReadNextAsync();
            return Math.Abs(response.Resource.First());
        }

        return decimal.Zero;
    }
}
```

Notice the use of the **Description** attribute throughout this code. This is important as it allows Semantic Kernel to use the associated GPT reference to understand what the method does and what is expected for each parameter. This allows the **Planner** to understand how to call the method.

Here the method expects to be given a **category**, **start date**, and **end date**. It will then use these values to query Cosmos. All in all this is very common code for applications performing queries against a Cosmos database. In a production application we would want to better handle the *connection* but, beyond that you would find code like this for Cosmos backed applications.

4. Finally we have to update **Program.cs** to inform Semantic Kernel the **CosmosPlugin** can be used. We do that by update our Kernel configuration to this:
```
var config = new ConfigurationBuilder()
    .AddUserSecrets<Config>()
    .Build();

var builder = Kernel.CreateBuilder();
builder.Services.AddSingleton<IConfiguration>(config);

builder.Plugins.AddFromType<CosmosPlugin>();
builder.AddAzureOpenAIChatCompletion(
    deploymentName: "gpt-4o-gs-deployment",
    endpoint: config["OpenAIEndpoint"],
    apiKey: config["OpenAIApiKey"]);
```

We added the call to `builder.Plugins.AddFromType<CosmosPlugin>();` which makes the Kernel aware of the plugin. And it will understand what it does and what it needs using the **Description** attribute values.

Let's run the program again, this time provide the request string: `How much was spent on Groceries in May 2024?`
This should produce the result: `1333.1599999999996`

Ok so this is really cool. If you debug the program you will find that **ByCategoryInDateRange** method is being called with the following:
 - category = 'Groceries'
 - startDate = 2024-05-01
 - endDate = 2024-05-31

 That is really cool. Using the GPT model, Semantic Kernel was able to deduce bits of information from the request and pass the appropriate parameter values. It used the **Description** values to determine what it needed.

 ### Format the Output
 So we are successfully querying Cosmos with the data from the request string. But, the output leaves a lot to be desired. What would be better is if our result was something like: `You spent $1333.16 on Groceries in May 2024".

 To do this, we can use a **Prompt-plugin** to ask GPT to reformat the string. **Prompt-pluigins** differ from *code plugins* because they dont feature code, instead specific files are included which control how a given prompt is sent to the GPT client.

 1. Create a new folder under the **Plugins** directory called **FormatterPlugin** or whatever name you decide
 2. For a given *function* in this plugin we create another folder. Create **FormatCategoryRequestOutput**

 3. We need to include two specific files here: **config.json** and **skprompt.txt**
 **config.json** controls the parameters used to call the GPT client. Use these contents for our application:
 ```
 {
  "schema": 1,
  "description": "Given a by category request for a time range, format the resulting monetary value",
  "execution_settings": {
    "default": {
      "max_tokens": 70,
      "temperature": 0.9,
      "top_p": 0.0,
      "presence_penalty": 0.0,
      "frequency_penalty": 0.0
    }
  },
  "input_variables": [
    {
      "name": "number",
      "description": "The value returned from to format",
      "default": "",
      "is_required": true
    },
    {
      "name": "category",
      "description": "The category for which the monetary value was requested",
      "default": "",
      "is_required": true
    },
    {
      "name": "time_period",
      "description": "The time period for which the monetary value was requested",
      "default": "",
      "is_required": true
    }
  ]
}
```
Once again, a *description* is provided which allows Semantic Kernel to understand what the plugin function does. Also, we specify *input_variables* as what variables we may want to pass to the plugin to be used in the prompt. Here we the **number** to format and the **category** and **time_period** to filter by. Each of these parameters has a description to allow Semantic Kernel to understand what they are.

**skprompt.txt** represents the template prompt to be used when calling GPT. This leverages *Prompt-engineering* concepts to indicate to the GPT client what it is being asked to do. Handlebar syntax can be used to reference the input variables specified in **config.json**. Use this for our example:
```
THE NUMBER {{ $number }} REPRESENTS THE SUM TOTAL FOR ALL TRANSACTIONS SPENT IN THE {{ $category}} CATEGORY
RETURN A STATEMENT THAT INCLUDES THE NUMBER {{ $number }} AND THE CATEGORY {{ $category }} AND TIME PERIOND {{ $time_period }} IN A FORMAT LIKE THIS EXAMPLE:
You spent $56 on Groceries in June 2024
```

My advice is to be very overt when constructing the prompt and include examples of what is being asked so GPT can better understand what kind of output you want.

Both of these files should be stored under the **FormatCategoryRequestOutput** folder:  
![Plugin files shown in VSCode](./readme_images/vscode-screen1.png)

4. Next we need to update **Program.cs** to include the Prompt-plugin. Below is the updated section. The call is quite different than what is done with *code-based* plugins.
```
var config = new ConfigurationBuilder()
    .AddUserSecrets<Config>()
    .Build();

var builder = Kernel.CreateBuilder();
builder.Services.AddSingleton<IConfiguration>(config);

builder.Plugins.AddFromType<CosmosPlugin>();
builder.Plugins.AddFromPromptDirectory(Path.Combine(Directory.GetCurrentDirectory(), "Plugins", "FormatterPlugin"));
builder.AddAzureOpenAIChatCompletion(
    deploymentName: "gpt-4o-gs-deployment",
    endpoint: config["OpenAIEndpoint"],
    apiKey: config["OpenAIApiKey"]);

var kernel = builder.Build();
```

We added `builder.Plugins.AddFromPromptDirectory(Path.Combine(Directory.GetCurrentDirectory(), "Plugins", "FormatterPlugin"));` and target the **PLUGIN** directory, not the function directory.

5. Run the application again with the request `How much was spent on Groceries in May 2024?`. My result is: `You spent $1333.16 on Groceries in May 2024.`

Much nicer. Our application works well and should handle more premutations. But how well does it handle some shortcuts in natural language. For example, what if I ask `How much was spent on Groceries last month?` I get an error, lets fix that.